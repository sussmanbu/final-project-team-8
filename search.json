[
  {
    "objectID": "Potential Graphs.html",
    "href": "Potential Graphs.html",
    "title": "Potential Graphs",
    "section": "",
    "text": "suppressMessages(library(readxl))\nsuppressMessages(library(tidyverse))\n\nacs_stuff &lt;- read.csv('dataset/ACS_Gender_and_Race.csv')\nincome_data &lt;- read.csv('dataset/dofl_income_clean.csv')\n\n\n# numeric for percentage\nacs_stuff$Male_P_Bach &lt;- as.numeric(gsub(\"%\", \"\", acs_stuff$Male_P_Bach))\n\nWarning: NAs introduced by coercion\n\nacs_stuff$Female_P_Bach &lt;- as.numeric(gsub(\"%\", \"\", acs_stuff$Female_P_Bach))\n\nWarning: NAs introduced by coercion\n\n# organizing raced percentage data\nacs_stuff %&gt;% group_by(Race) %&gt;%\n  summarize(\n    bach_perc_male = sum(Male_P_Bach, na.rm = TRUE) / n(),\n    bach_perc_female = sum(Female_P_Bach, na.rm = TRUE) / n(),\n    prop_m_to_f = bach_perc_female/bach_perc_male\n  )\n\n# A tibble: 9 × 4\n  Race                               bach_perc_male bach_perc_female prop_m_to_f\n  &lt;chr&gt;                                       &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n1 American Indian or Alaska Native …           13.8             17.2       1.25 \n2 Asian Alone                                  56.6             50.5       0.892\n3 Black Alone                                  20.8             24.8       1.19 \n4 Hispanic or Latino Origin                    17.4             21.2       1.22 \n5 Native Hawaiian and Other Pacific…           11.2             11.9       1.07 \n6 Some other race Alone                        13.8             16.9       1.23 \n7 Two or more races                            27.7             32.1       1.16 \n8 White Alone                                  34.3             36.0       1.05 \n9 White alone, not Hispanic or Lati…           35.8             37.2       1.04 \n\n\n\n# numeric for median data\nview(income_data)"
  },
  {
    "objectID": "posts/blog-post-6/blog-post-6.html",
    "href": "posts/blog-post-6/blog-post-6.html",
    "title": "Team 8 Final Project",
    "section": "",
    "text": "Blog Post 6\n04/21/2024\nWe have begun to consider different interactive approaches for our final product, and one of the widgets that stuck out to us the most was RPivottables, given the large amount of variables in our data, having a way for users to quickly filter through it to find what they are looking for would be ideal.\nIn general, we have data that is differentiated solely by gender and data that is differentiated by both gender and race. We hope to isolate specific races and see how the gender gap for Bachelor Degree accomplishment differs from both the general census data (ie “white only vs total US”) and the between the different races (ie “white only vs black only”)\nTo visualize this we think using the ggiraph widgets would be useful. We can use ggiraph to make ggplot graphs interactive. With the widget we can have general scatter plots for Time vs percentage of women that achieved at least a Bachelor’s Degree\nIn terms of a thesis, we are beginning to close in on a definite statement but need more time with our data to present anything concrete. We have narrowed down our response variables to college education (% of Bachelor’s Degrees) and income. Based on casual observations, we can see the disparity between men and women in terms of these concepts. However, we need to conduct further analysis before we can decide exactly what it is that we are trying to show with all of our large collection of data. We plan to have our thesis statement fully fleshed out by the end of the week."
  },
  {
    "objectID": "posts/blog-post-4/Blog-post-4.html",
    "href": "posts/blog-post-4/Blog-post-4.html",
    "title": "Team 8 Final Project",
    "section": "",
    "text": "Blog Post 4\n04/08/2024\nWhile looking through our ACS data, we saw consistent, yearly data about the percentage of women that acquired at least a Bachelor’s degree in each of the 50 states and in Puerto Rico. We hope to be able to predict the percentage of women of a certain race that acquired at least a Bachelor’s Degree in a given year. Here, years and race would be predictor variables for the percentage of women that acquired at least a Bachelor’s Degree (response variable). Regarding regression models, we think a logistical regression would not fit as our predictor variables are not dichotomous or binary (they have more than two outputs). Therefore, we think a linear regression will fit more nicely.\nInterestingly, when looking at our cleaned data, we have already seen some interesting differences that we are excited to explore deeper in our analysis. Last but not least, we have also begun discussing strategies regarding data visualization. We have not come to any conclusions, but we hope to be able to incorporate some gendered differences (for given races). We hope to hone in on our ideas this coming week.\n{An excerpt of cleaned data can be found below}\n{Male, White-alone, Across all U.S. States + Puerto Rico (Status: Cleaned)}\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tibble)\n\nacs_data &lt;- read_excel(\"dataset/acsfinale.xlsx\")\n\nNew names:\n• `Alabama` -&gt; `Alabama...3`\n• `Alabama` -&gt; `Alabama...4`\n• `Alabama` -&gt; `Alabama...5`\n• `Alabama` -&gt; `Alabama...6`\n• `Alabama` -&gt; `Alabama...7`\n• `Alabama` -&gt; `Alabama...8`\n• `Alabama` -&gt; `Alabama...9`\n• `Alabama` -&gt; `Alabama...10`\n• `Alabama` -&gt; `Alabama...11`\n• `Alabama` -&gt; `Alabama...12`\n• `Alabama` -&gt; `Alabama...13`\n• `Alabama` -&gt; `Alabama...14`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n• `` -&gt; `...58`\n• `` -&gt; `...59`\n• `` -&gt; `...60`\n• `` -&gt; `...61`\n• `` -&gt; `...62`\n• `` -&gt; `...64`\n• `` -&gt; `...65`\n• `` -&gt; `...66`\n• `` -&gt; `...67`\n• `` -&gt; `...68`\n• `` -&gt; `...69`\n• `` -&gt; `...70`\n• `` -&gt; `...71`\n• `` -&gt; `...72`\n• `` -&gt; `...73`\n• `` -&gt; `...74`\n• `` -&gt; `...76`\n• `` -&gt; `...77`\n• `` -&gt; `...78`\n• `` -&gt; `...79`\n• `` -&gt; `...80`\n• `` -&gt; `...81`\n• `` -&gt; `...82`\n• `` -&gt; `...83`\n• `` -&gt; `...84`\n• `` -&gt; `...85`\n• `` -&gt; `...86`\n• `` -&gt; `...88`\n• `` -&gt; `...89`\n• `` -&gt; `...90`\n• `` -&gt; `...91`\n• `` -&gt; `...92`\n• `` -&gt; `...93`\n• `` -&gt; `...94`\n• `` -&gt; `...95`\n• `` -&gt; `...96`\n• `` -&gt; `...97`\n• `` -&gt; `...98`\n• `` -&gt; `...100`\n• `` -&gt; `...101`\n• `` -&gt; `...102`\n• `` -&gt; `...103`\n• `` -&gt; `...104`\n• `` -&gt; `...105`\n• `` -&gt; `...106`\n• `` -&gt; `...107`\n• `` -&gt; `...108`\n• `` -&gt; `...109`\n• `` -&gt; `...110`\n• `` -&gt; `...112`\n• `` -&gt; `...113`\n• `` -&gt; `...114`\n• `` -&gt; `...115`\n• `` -&gt; `...116`\n• `` -&gt; `...117`\n• `` -&gt; `...118`\n• `` -&gt; `...119`\n• `` -&gt; `...120`\n• `` -&gt; `...121`\n• `` -&gt; `...122`\n• `` -&gt; `...124`\n• `` -&gt; `...125`\n• `` -&gt; `...126`\n• `` -&gt; `...127`\n• `` -&gt; `...128`\n• `` -&gt; `...129`\n• `` -&gt; `...130`\n• `` -&gt; `...131`\n• `` -&gt; `...132`\n• `` -&gt; `...133`\n• `` -&gt; `...134`\n• `` -&gt; `...136`\n• `` -&gt; `...137`\n• `` -&gt; `...138`\n• `` -&gt; `...139`\n• `` -&gt; `...140`\n• `` -&gt; `...141`\n• `` -&gt; `...142`\n• `` -&gt; `...143`\n• `` -&gt; `...144`\n• `` -&gt; `...145`\n• `` -&gt; `...146`\n• `` -&gt; `...148`\n• `` -&gt; `...149`\n• `` -&gt; `...150`\n• `` -&gt; `...151`\n• `` -&gt; `...152`\n• `` -&gt; `...153`\n• `` -&gt; `...154`\n• `` -&gt; `...155`\n• `` -&gt; `...156`\n• `` -&gt; `...157`\n• `` -&gt; `...158`\n• `` -&gt; `...160`\n• `` -&gt; `...161`\n• `` -&gt; `...162`\n• `` -&gt; `...163`\n• `` -&gt; `...164`\n• `` -&gt; `...165`\n• `` -&gt; `...166`\n• `` -&gt; `...167`\n• `` -&gt; `...168`\n• `` -&gt; `...169`\n• `` -&gt; `...170`\n• `` -&gt; `...172`\n• `` -&gt; `...173`\n• `` -&gt; `...174`\n• `` -&gt; `...175`\n• `` -&gt; `...176`\n• `` -&gt; `...177`\n• `` -&gt; `...178`\n• `` -&gt; `...179`\n• `` -&gt; `...180`\n• `` -&gt; `...181`\n• `` -&gt; `...182`\n• `` -&gt; `...184`\n• `` -&gt; `...185`\n• `` -&gt; `...186`\n• `` -&gt; `...187`\n• `` -&gt; `...188`\n• `` -&gt; `...189`\n• `` -&gt; `...190`\n• `` -&gt; `...191`\n• `` -&gt; `...192`\n• `` -&gt; `...193`\n• `` -&gt; `...194`\n• `` -&gt; `...196`\n• `` -&gt; `...197`\n• `` -&gt; `...198`\n• `` -&gt; `...199`\n• `` -&gt; `...200`\n• `` -&gt; `...201`\n• `` -&gt; `...202`\n• `` -&gt; `...203`\n• `` -&gt; `...204`\n• `` -&gt; `...205`\n• `` -&gt; `...206`\n• `` -&gt; `...208`\n• `` -&gt; `...209`\n• `` -&gt; `...210`\n• `` -&gt; `...211`\n• `` -&gt; `...212`\n• `` -&gt; `...213`\n• `` -&gt; `...214`\n• `` -&gt; `...215`\n• `` -&gt; `...216`\n• `` -&gt; `...217`\n• `` -&gt; `...218`\n• `` -&gt; `...220`\n• `` -&gt; `...221`\n• `` -&gt; `...222`\n• `` -&gt; `...223`\n• `` -&gt; `...224`\n• `` -&gt; `...225`\n• `` -&gt; `...226`\n• `` -&gt; `...227`\n• `` -&gt; `...228`\n• `` -&gt; `...229`\n• `` -&gt; `...230`\n• `` -&gt; `...232`\n• `` -&gt; `...233`\n• `` -&gt; `...234`\n• `` -&gt; `...235`\n• `` -&gt; `...236`\n• `` -&gt; `...237`\n• `` -&gt; `...238`\n• `` -&gt; `...239`\n• `` -&gt; `...240`\n• `` -&gt; `...241`\n• `` -&gt; `...242`\n• `` -&gt; `...244`\n• `` -&gt; `...245`\n• `` -&gt; `...246`\n• `` -&gt; `...247`\n• `` -&gt; `...248`\n• `` -&gt; `...249`\n• `` -&gt; `...250`\n• `` -&gt; `...251`\n• `` -&gt; `...252`\n• `` -&gt; `...253`\n• `` -&gt; `...254`\n• `` -&gt; `...256`\n• `` -&gt; `...257`\n• `` -&gt; `...258`\n• `` -&gt; `...259`\n• `` -&gt; `...260`\n• `` -&gt; `...261`\n• `` -&gt; `...262`\n• `` -&gt; `...263`\n• `` -&gt; `...264`\n• `` -&gt; `...265`\n• `` -&gt; `...266`\n• `` -&gt; `...268`\n• `` -&gt; `...269`\n• `` -&gt; `...270`\n• `` -&gt; `...271`\n• `` -&gt; `...272`\n• `` -&gt; `...273`\n• `` -&gt; `...274`\n• `` -&gt; `...275`\n• `` -&gt; `...276`\n• `` -&gt; `...277`\n• `` -&gt; `...278`\n• `` -&gt; `...280`\n• `` -&gt; `...281`\n• `` -&gt; `...282`\n• `` -&gt; `...283`\n• `` -&gt; `...284`\n• `` -&gt; `...285`\n• `` -&gt; `...286`\n• `` -&gt; `...287`\n• `` -&gt; `...288`\n• `` -&gt; `...289`\n• `` -&gt; `...290`\n• `` -&gt; `...292`\n• `` -&gt; `...293`\n• `` -&gt; `...294`\n• `` -&gt; `...295`\n• `` -&gt; `...296`\n• `` -&gt; `...297`\n• `` -&gt; `...298`\n• `` -&gt; `...299`\n• `` -&gt; `...300`\n• `` -&gt; `...301`\n• `` -&gt; `...302`\n• `` -&gt; `...304`\n• `` -&gt; `...305`\n• `` -&gt; `...306`\n• `` -&gt; `...307`\n• `` -&gt; `...308`\n• `` -&gt; `...309`\n• `` -&gt; `...310`\n• `` -&gt; `...311`\n• `` -&gt; `...312`\n• `` -&gt; `...313`\n• `` -&gt; `...314`\n• `` -&gt; `...316`\n• `` -&gt; `...317`\n• `` -&gt; `...318`\n• `` -&gt; `...319`\n• `` -&gt; `...320`\n• `` -&gt; `...321`\n• `` -&gt; `...322`\n• `` -&gt; `...323`\n• `` -&gt; `...324`\n• `` -&gt; `...325`\n• `` -&gt; `...326`\n• `` -&gt; `...328`\n• `` -&gt; `...329`\n• `` -&gt; `...330`\n• `` -&gt; `...331`\n• `` -&gt; `...332`\n• `` -&gt; `...333`\n• `` -&gt; `...334`\n• `` -&gt; `...335`\n• `` -&gt; `...336`\n• `` -&gt; `...337`\n• `` -&gt; `...338`\n• `` -&gt; `...340`\n• `` -&gt; `...341`\n• `` -&gt; `...342`\n• `` -&gt; `...343`\n• `` -&gt; `...344`\n• `` -&gt; `...345`\n• `` -&gt; `...346`\n• `` -&gt; `...347`\n• `` -&gt; `...348`\n• `` -&gt; `...349`\n• `` -&gt; `...350`\n• `` -&gt; `...352`\n• `` -&gt; `...353`\n• `` -&gt; `...354`\n• `` -&gt; `...355`\n• `` -&gt; `...356`\n• `` -&gt; `...357`\n• `` -&gt; `...358`\n• `` -&gt; `...359`\n• `` -&gt; `...360`\n• `` -&gt; `...361`\n• `` -&gt; `...362`\n• `` -&gt; `...364`\n• `` -&gt; `...365`\n• `` -&gt; `...366`\n• `` -&gt; `...367`\n• `` -&gt; `...368`\n• `` -&gt; `...369`\n• `` -&gt; `...370`\n• `` -&gt; `...371`\n• `` -&gt; `...372`\n• `` -&gt; `...373`\n• `` -&gt; `...374`\n• `` -&gt; `...376`\n• `` -&gt; `...377`\n• `` -&gt; `...378`\n• `` -&gt; `...379`\n• `` -&gt; `...380`\n• `` -&gt; `...381`\n• `` -&gt; `...382`\n• `` -&gt; `...383`\n• `` -&gt; `...384`\n• `` -&gt; `...385`\n• `` -&gt; `...386`\n• `` -&gt; `...388`\n• `` -&gt; `...389`\n• `` -&gt; `...390`\n• `` -&gt; `...391`\n• `` -&gt; `...392`\n• `` -&gt; `...393`\n• `` -&gt; `...394`\n• `` -&gt; `...395`\n• `` -&gt; `...396`\n• `` -&gt; `...397`\n• `` -&gt; `...398`\n• `` -&gt; `...400`\n• `` -&gt; `...401`\n• `` -&gt; `...402`\n• `` -&gt; `...403`\n• `` -&gt; `...404`\n• `` -&gt; `...405`\n• `` -&gt; `...406`\n• `` -&gt; `...407`\n• `` -&gt; `...408`\n• `` -&gt; `...409`\n• `` -&gt; `...410`\n• `` -&gt; `...412`\n• `` -&gt; `...413`\n• `` -&gt; `...414`\n• `` -&gt; `...415`\n• `` -&gt; `...416`\n• `` -&gt; `...417`\n• `` -&gt; `...418`\n• `` -&gt; `...419`\n• `` -&gt; `...420`\n• `` -&gt; `...421`\n• `` -&gt; `...422`\n• `` -&gt; `...424`\n• `` -&gt; `...425`\n• `` -&gt; `...426`\n• `` -&gt; `...427`\n• `` -&gt; `...428`\n• `` -&gt; `...429`\n• `` -&gt; `...430`\n• `` -&gt; `...431`\n• `` -&gt; `...432`\n• `` -&gt; `...433`\n• `` -&gt; `...434`\n• `` -&gt; `...436`\n• `` -&gt; `...437`\n• `` -&gt; `...438`\n• `` -&gt; `...439`\n• `` -&gt; `...440`\n• `` -&gt; `...441`\n• `` -&gt; `...442`\n• `` -&gt; `...443`\n• `` -&gt; `...444`\n• `` -&gt; `...445`\n• `` -&gt; `...446`\n• `` -&gt; `...448`\n• `` -&gt; `...449`\n• `` -&gt; `...450`\n• `` -&gt; `...451`\n• `` -&gt; `...452`\n• `` -&gt; `...453`\n• `` -&gt; `...454`\n• `` -&gt; `...455`\n• `` -&gt; `...456`\n• `` -&gt; `...457`\n• `` -&gt; `...458`\n• `` -&gt; `...460`\n• `` -&gt; `...461`\n• `` -&gt; `...462`\n• `` -&gt; `...463`\n• `` -&gt; `...464`\n• `` -&gt; `...465`\n• `` -&gt; `...466`\n• `` -&gt; `...467`\n• `` -&gt; `...468`\n• `` -&gt; `...469`\n• `` -&gt; `...470`\n• `` -&gt; `...472`\n• `` -&gt; `...473`\n• `` -&gt; `...474`\n• `` -&gt; `...475`\n• `` -&gt; `...476`\n• `` -&gt; `...477`\n• `` -&gt; `...478`\n• `` -&gt; `...479`\n• `` -&gt; `...480`\n• `` -&gt; `...481`\n• `` -&gt; `...482`\n• `` -&gt; `...484`\n• `` -&gt; `...485`\n• `` -&gt; `...486`\n• `` -&gt; `...487`\n• `` -&gt; `...488`\n• `` -&gt; `...489`\n• `` -&gt; `...490`\n• `` -&gt; `...491`\n• `` -&gt; `...492`\n• `` -&gt; `...493`\n• `` -&gt; `...494`\n• `` -&gt; `...496`\n• `` -&gt; `...497`\n• `` -&gt; `...498`\n• `` -&gt; `...499`\n• `` -&gt; `...500`\n• `` -&gt; `...501`\n• `` -&gt; `...502`\n• `` -&gt; `...503`\n• `` -&gt; `...504`\n• `` -&gt; `...505`\n• `` -&gt; `...506`\n• `` -&gt; `...508`\n• `` -&gt; `...509`\n• `` -&gt; `...510`\n• `` -&gt; `...511`\n• `` -&gt; `...512`\n• `` -&gt; `...513`\n• `` -&gt; `...514`\n• `` -&gt; `...515`\n• `` -&gt; `...516`\n• `` -&gt; `...517`\n• `` -&gt; `...518`\n• `` -&gt; `...520`\n• `` -&gt; `...521`\n• `` -&gt; `...522`\n• `` -&gt; `...523`\n• `` -&gt; `...524`\n• `` -&gt; `...525`\n• `` -&gt; `...526`\n• `` -&gt; `...527`\n• `` -&gt; `...528`\n• `` -&gt; `...529`\n• `` -&gt; `...530`\n• `` -&gt; `...532`\n• `` -&gt; `...533`\n• `` -&gt; `...534`\n• `` -&gt; `...535`\n• `` -&gt; `...536`\n• `` -&gt; `...537`\n• `` -&gt; `...538`\n• `` -&gt; `...539`\n• `` -&gt; `...540`\n• `` -&gt; `...541`\n• `` -&gt; `...542`\n• `` -&gt; `...544`\n• `` -&gt; `...545`\n• `` -&gt; `...546`\n• `` -&gt; `...547`\n• `` -&gt; `...548`\n• `` -&gt; `...549`\n• `` -&gt; `...550`\n• `` -&gt; `...551`\n• `` -&gt; `...552`\n• `` -&gt; `...553`\n• `` -&gt; `...554`\n• `` -&gt; `...556`\n• `` -&gt; `...557`\n• `` -&gt; `...558`\n• `` -&gt; `...559`\n• `` -&gt; `...560`\n• `` -&gt; `...561`\n• `` -&gt; `...562`\n• `` -&gt; `...563`\n• `` -&gt; `...564`\n• `` -&gt; `...565`\n• `` -&gt; `...566`\n• `` -&gt; `...568`\n• `` -&gt; `...569`\n• `` -&gt; `...570`\n• `` -&gt; `...571`\n• `` -&gt; `...572`\n• `` -&gt; `...573`\n• `` -&gt; `...574`\n• `` -&gt; `...575`\n• `` -&gt; `...576`\n• `` -&gt; `...577`\n• `` -&gt; `...578`\n• `` -&gt; `...580`\n• `` -&gt; `...581`\n• `` -&gt; `...582`\n• `` -&gt; `...583`\n• `` -&gt; `...584`\n• `` -&gt; `...585`\n• `` -&gt; `...586`\n• `` -&gt; `...587`\n• `` -&gt; `...588`\n• `` -&gt; `...589`\n• `` -&gt; `...590`\n• `` -&gt; `...592`\n• `` -&gt; `...593`\n• `` -&gt; `...594`\n• `` -&gt; `...595`\n• `` -&gt; `...596`\n• `` -&gt; `...597`\n• `` -&gt; `...598`\n• `` -&gt; `...599`\n• `` -&gt; `...600`\n• `` -&gt; `...601`\n• `` -&gt; `...602`\n• `` -&gt; `...604`\n• `` -&gt; `...605`\n• `` -&gt; `...606`\n• `` -&gt; `...607`\n• `` -&gt; `...608`\n• `` -&gt; `...609`\n• `` -&gt; `...610`\n• `` -&gt; `...611`\n• `` -&gt; `...612`\n• `` -&gt; `...613`\n• `` -&gt; `...614`\n• `` -&gt; `...616`\n• `` -&gt; `...617`\n• `` -&gt; `...618`\n• `` -&gt; `...619`\n• `` -&gt; `...620`\n• `` -&gt; `...621`\n• `` -&gt; `...622`\n• `` -&gt; `...623`\n• `` -&gt; `...624`\n• `` -&gt; `...625`\n• `` -&gt; `...626`\n\n# Alabama\nus_whitealone_data &lt;- filter(acs_data, Label == \"Bachelor's degree or higher*\")\nbdh_whitealone_alabama &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`Alabama...9`,\n  Race = \"White Alone\")\n\n# Alaska\nbdh_whitealone_alaska &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...21`,\n  Race = \"White Alone\"\n)\n\n\n# Arizona\nbdh_whitealone_arizona &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...33`,\n  Race = \"White Alone\"\n)\n\n# Arkansas\nbdh_whitealone_arkansas &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...45`,\n  Race = \"White Alone\"\n)\n\n# California\nbdh_whitealone_california &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...57`,\n  Race = \"White Alone\"\n)\n\n# Colorado\nbdh_whitealone_colorado &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...69`,\n  Race = \"White Alone\"\n)\n\n# Connecticut\nbdh_whitealone_connecticut &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...81`,\n  Race = \"White Alone\"\n)\n\n# Delaware\nbdh_whitealone_delaware &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...93`,\n  Race = \"White Alone\"\n)\n\n# Florida\nbdh_whitealone_florida &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...105`,\n  Race = \"White Alone\"\n)\n\n# Georgia\nbdh_whitealone_georgia &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...117`,\n  Race = \"White Alone\"\n)\n\n# Hawaii\nbdh_whitealone_hawaii &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...129`,\n  Race = \"White Alone\"\n)\n\n# Idaho\nbdh_whitealone_idaho &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...141`,\n  Race = \"White Alone\"\n)\n\n# Illinois\nbdh_whitealone_illinois &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...153`,\n  Race = \"White Alone\"\n)\n\n# Indiana\nbdh_whitealone_indiana &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...165`,\n  Race = \"White Alone\"\n)\n\n# Iowa\nbdh_whitealone_iowa &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...177`,\n  Race = \"White Alone\"\n)\n\n# Kansas\nbdh_whitealone_kansas &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...189`,\n  Race = \"White Alone\"\n)\n\n# Kentucky\nbdh_whitealone_kentucky &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...201`,\n  Race = \"White Alone\"\n)\n\n# Louisiana\nbdh_whitealone_louisiana &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...213`,\n  Race = \"White Alone\"\n)\n\n# Maine\nbdh_whitealone_maine &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...225`,\n  Race = \"White Alone\"\n)\n\n# Maryland\nbdh_whitealone_maryland &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...237`,\n  Race = \"White Alone\"\n)\n\n# Massachusetts\nbdh_whitealone_massachusetts &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...249`,\n  Race = \"White Alone\"\n)\n\n# Michigan\nbdh_whitealone_michigan &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...261`,\n  Race = \"White Alone\"\n)\n\n# Minnesota\nbdh_whitealone_minnesota &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...273`,\n  Race = \"White Alone\"\n)\n\n# Mississippi\nbdh_whitealone_mississippi &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...285`,\n  Race = \"White Alone\"\n)\n\n# Missouri\nbdh_whitealone_missouri &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...397`,\n  Race = \"White Alone\"\n)\n\n# Montana\nbdh_whitealone_montana &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...309`,\n  Race = \"White Alone\"\n)\n\n# Nebraska\nbdh_whitealone_nebraska &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...321`,\n  Race = \"White Alone\"\n)\n\n# Nevada\nbdh_whitealone_nevada &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...333`,\n  Race = \"White Alone\"\n)\n\n# New Hampshire\nbdh_whitealone_newhampshire &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...345`,\n  Race = \"White Alone\"\n)\n\n# New Jersey\nbdh_whitealone_newjersey &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...357`,\n  Race = \"White Alone\"\n)\n\n# New Mexico\nbdh_whitealone_newmexico &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...369`,\n  Race = \"White Alone\"\n)\n\n# New York\nbdh_whitealone_newyork &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...381`,\n  Race = \"White Alone\"\n)\n\n# North Carolina\nbdh_whitealone_northcarolina &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...393`,\n  Race = \"White Alone\"\n)\n\n# North Dakota\nbdh_whitealone_northdakota &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...405`,\n  Race = \"White Alone\"\n)\n\n# Ohio\nbdh_whitealone_ohio &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...417`,\n  Race = \"White Alone\"\n)\n\n# Oklahoma\nbdh_whitealone_oklahoma &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...429`,\n  Race = \"White Alone\"\n)\n\n# Oregon\nbdh_whitealone_oregon &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...441`,\n  Race = \"White Alone\"\n)\n\n# Pennsylvania\nbdh_whitealone_pennsylvania &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...453`,\n  Race = \"White Alone\"\n)\n\n# Rhode Island\nbdh_whitealone_rhodeisland &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...465`,\n  Race = \"White Alone\"\n)\n\n# South Carolina\nbdh_whitealone_southcarolina &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...477`,\n  Race = \"White Alone\"\n)\n\n# South Dakota\nbdh_whitealone_southdakota &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...489`,\n  Race = \"White Alone\"\n)\n\n# Tennessee\nbdh_whitealone_tennessee &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...501`,\n  Race = \"White Alone\"\n)\n\n# Texas\nbdh_whitealone_texas &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...513`,\n  Race = \"White Alone\"\n)\n\n# Utah\nbdh_whitealone_utah &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...525`,\n  Race = \"White Alone\"\n)\n\n# Vermont\nbdh_whitealone_vermont &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...537`,\n  Race = \"White Alone\"\n)\n\n# Virginia\nbdh_whitealone_virginia &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...549`,\n  Race = \"White Alone\"\n)\n\n# Washington\nbdh_whitealone_washington &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...561`,\n  Race = \"White Alone\"\n)\n\n# West Virginia\nbdh_whitealone_westvirginia &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...573`,\n  Race = \"White Alone\"\n)\n\n# Wisconsin\nbdh_whitealone_wisconsin &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...585`,\n Race = \"White Alone\"\n)\n\n# Wyoming\nbdh_whitealone_wyoming &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...597`,\n  Race = \"White Alone\"\n)\n\n# Puerto Rico\nbdh_whitealone_puertorico &lt;- tibble(\n  Years = us_whitealone_data$Years,\n  Percentage_men = us_whitealone_data$`...597`,\n  Race = \"White Alone\"\n)\n\n# Combine all tibbles into one\ncombined_data_malewhitealone &lt;- bind_rows(\n  bdh_whitealone_alabama,\n  bdh_whitealone_alaska,\n  bdh_whitealone_arizona,\n  bdh_whitealone_arkansas,\n  bdh_whitealone_california,\n  bdh_whitealone_colorado,\n  bdh_whitealone_connecticut,\n  bdh_whitealone_delaware,\n  bdh_whitealone_florida,\n  bdh_whitealone_georgia,\n  bdh_whitealone_hawaii,\n  bdh_whitealone_idaho,\n  bdh_whitealone_illinois,\n  bdh_whitealone_indiana,\n  bdh_whitealone_iowa,\n  bdh_whitealone_kansas,\n  bdh_whitealone_kentucky,\n  bdh_whitealone_louisiana,\n  bdh_whitealone_maine,\n  bdh_whitealone_maryland,\n  bdh_whitealone_massachusetts,\n  bdh_whitealone_michigan,\n  bdh_whitealone_minnesota,\n  bdh_whitealone_mississippi,\n  bdh_whitealone_missouri,\n  bdh_whitealone_montana,\n  bdh_whitealone_nebraska,\n  bdh_whitealone_nevada,\n  bdh_whitealone_newhampshire,\n  bdh_whitealone_newjersey,\n  bdh_whitealone_newmexico,\n  bdh_whitealone_newyork,\n  bdh_whitealone_northcarolina,\n  bdh_whitealone_northdakota,\n  bdh_whitealone_ohio,\n  bdh_whitealone_oklahoma,\n  bdh_whitealone_oregon,\n  bdh_whitealone_pennsylvania,\n  bdh_whitealone_rhodeisland,\n  bdh_whitealone_southcarolina,\n  bdh_whitealone_southdakota,\n  bdh_whitealone_tennessee,\n  bdh_whitealone_texas,\n  bdh_whitealone_utah,\n  bdh_whitealone_vermont,\n  bdh_whitealone_virginia,\n  bdh_whitealone_washington,\n  bdh_whitealone_westvirginia,\n  bdh_whitealone_wisconsin,\n  bdh_whitealone_wyoming,\n  bdh_whitealone_puertorico\n)\n\nprint(combined_data_malewhitealone)\n\n# A tibble: 408 × 3\n   Years Percentage_men Race       \n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;      \n 1  2022 31.3%          White Alone\n 2  2021 30.4%          White Alone\n 3  2020 28.9%          White Alone\n 4  2019 28.5%          White Alone\n 5  2018 28.4%          White Alone\n 6  2017 28.7%          White Alone\n 7  2016 27.8%          White Alone\n 8  2015 26.6%          White Alone\n 9  2022 34.1%          White Alone\n10  2021 33.8%          White Alone\n# ℹ 398 more rows"
  },
  {
    "objectID": "posts/2024-03-22-blog-post-2-/blog-post-2.html",
    "href": "posts/2024-03-22-blog-post-2-/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Blog Post 2 - Reviewing Feedback and Data Exploration \nThroughout this week, we reviewed the feedback on our datasets. We decided that we would use our datasets on death rates via suicide and unemployment as supplementary evidence later on in our project. These two datasets are quite messy to use and proved challenging to sort/clean them to provide a compelling argument. We found a third supporting dataset from the World Bank on Gender. We also decided to focus on the ACS data on Educational Attainment concerning Gender and Race (i.e., the Professor also told us how to extract data from ACS directly via R). \nFrom the ACS dataset, we brainstormed some ideas we could further explore. Firstly, we want to give a brief overview of the data we collected by exhibiting trends over time (i.e., showing poverty rates over time, median earnings of women from different racial backgrounds), and see if there are improvements or regressions in the data. Secondly, we also decided to conduct a gender-pay gap analysis by analyzing median earnings for women and men within each racial group based on their educational attainment. Relating to education, we also want to investigate the educational attainment of genders concerning race by seeing the percentage of women in different levels of education. Lastly, to further explore the disparities between genders, we want to see the poverty rates of women in each racial group, based on educational attainment. Run the chunk below to view the ACS dataset: \n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nNew names:\n• `` -&gt; `...1`\n\n\n\n\n\n\n##ACS Educational Attainment Dataset:\ndata1 = read_csv('dataset/ACS_dataset.csv')\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 392 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ;;Alabama;;;;;;;;;;;;Alaska;;;;;;;;;;;;Arizona;;;;;;;;;;;;Arkansas;...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nview(data1)\n\nThe World Bank dataset encapsulates a lot of data for many different categories including age-dependency ratios, population and birth rates, wage, and education. However, the World Bank data is not faceted by race. Additionally, there are also many NAs on certain measurements. We will probably use this data to compare with the ACS data to see averages and general correlations. Run the chunk below to view the World Bank Gender dataset:\n\n##World Bank Gender Dataset: \nlibrary(tidyverse)\ndata2 = read_csv('dataset/USA_Gender_Stats.csv')\n\nNew names:\nRows: 1153 Columns: 70\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): Country.Name, Country.Code, Indicator.Name, Indicator.Code dbl (65): ...1,\nX1960, X1961, X1962, X1963, X1964, X1965, X1966, X1967, X196... lgl (1): X\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nview(data2)"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "playing with data/figure_analysis.html",
    "href": "playing with data/figure_analysis.html",
    "title": "Team 8 Final Project",
    "section": "",
    "text": "Starting to Consider What Figures to Create\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nEmp_wb = read_csv('dataset/World_Bank_Employment_Data.csv', show_col_types = F)\nEduc_wb = read_csv('dataset/World_Bank_Educ_Data.csv', show_col_types = F)\n\n\nEmp_wb$Year &lt;- as.Date(as.character(Emp_wb$Year), format = \"%Y\") \n\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidycensus))\n\nWarning: package 'tidycensus' was built under R version 4.3.3\n\nsuppressPackageStartupMessages(library(sf))\n\nWarning: package 'sf' was built under R version 4.3.3\n\nv2020 &lt;- load_variables(2020, \"acs5\", cache = TRUE)\n\nconcepts &lt;- v2020 |&gt;\n  filter(str_detect(concept, '^MEDIAN INCOME'))\n\n### Would like to analyze B19326_004 and B19326_005 (They were a dud, most likely skewed due to part-time workers)\n\n### poverty data from ACS\npoverty_test &lt;- get_acs(\n  geography = 'state',\n  variables = 'B17001A_001',\n  year = 2020,\n  geometry = TRUE\n)\n\nGetting data from the 2016-2020 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n### Looking at C24040_002 and C24040_029 now which are\n##SEX BY INDUSTRY FOR THE FULL-TIME, YEAR-ROUND CIVILIAN EMPLOYED POPULATION 16 YEARS AND OVER for male and female\n\n\nvars_df &lt;- v2020 |&gt;\n  filter(name == 'C24040_002' | name == 'C24040_029') |&gt;\n  mutate(\n    variable = name,\n    gender = if_else(\n      str_detect(label, 'Male'),\n      'Male',\n      'Female'\n    ),\n    .keep = 'none'\n  )\n\ngendered_employment &lt;- get_acs(\n  geography = 'state',\n  variables = vars_df |&gt; pull(variable),\n  year = 2020,\n  geometry = TRUE\n) %&gt;% \n  left_join(vars_df) %&gt;%\n  select(-moe, -variable, -GEOID) \n\nGetting data from the 2016-2020 5-year ACS\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nJoining with `by = join_by(variable)`\n\n### name is state data\n\ngendered_employment_split &lt;- gendered_employment |&gt;\n  pivot_wider(names_from = 'gender', values_from = 'estimate')\n  \n\ntotal_employment &lt;- gendered_employment |&gt;\n  group_by(geometry) |&gt;\n  summarise(total = sum(estimate))\n\ngendered_employment_final &lt;- st_join(gendered_employment, total_employment, join = st_equals)\n\n\ngendered_employment_split |&gt;\n  ggplot(aes(fill =  `Male`/`Female`)) + \n  geom_sf() +\n  coord_sf(crs = 5070) +\n  scale_fill_viridis_c(option = 'plasma')+\n  theme_minimal()\n\n\n\n### this creates two maps, which are representative of the proportion of full time, year round civilians in each state faceted by gender. I'm not sure if this is the best approach to data presentation, and I am open to other ideas\n\n\nincome_data = read_csv(\"dataset/Income_dofl.csv\")\n\nRows: 780 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): In / Out of New Methodology Implemented, In / Out of New Processing...\ndbl (1): Year\nnum (1): Measure Values\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n### median income data split by race\n\nincome_data_clean &lt;- income_data|&gt;\n  filter(Year != 2013 | `In / Out of New Methodology Implemented` == 'In')|&gt;\n  filter(Year != 2017 | `In / Out of New Methodology Implemented` == 'In')|&gt;\n  select(-c(1,2)) |&gt;\n  group_by(`Measure Names`) |&gt;\n  pivot_wider(names_from = `Measure Names`, values_from = `Measure Values`) |&gt; arrange(desc(Year))\n\n\nlibrary(tidycensus)\n\n\ndesired_vars = c('B17001A_032','B17001A_003','B17001A_017', 'B17001A_046','B17001B_003','B17001B_017','B17001B_032','B17001B_046','B17001C_003','B17001C_017','B17001C_032','B17001C_046','B17001D_003','B17001D_017','B17001D_032','B17001D_046','B17001E_003','B17001E_017','B17001E_032','B17001E_046','B17001F_003','B17001F_017','B17001F_032','B17001F_046','B17001G_003','B17001G_017','B17001G_032','B17001G_046','B17001H_003','B17001H_017','B17001H_032','B17001H_046','B17001I_003','B17001I_017','B17001I_032','B17001I_046')\n\nv2020 &lt;- load_variables(2020, \"acs5\", cache = TRUE)\n\npoverty_data &lt;- v2020 |&gt;\n  filter(name %in% desired_vars) |&gt;\n  mutate(\n    variable = name,\n    gender = if_else(\n      str_detect(label, 'Male'),\n      'Male',\n      'Female'\n      ),\n   race = str_extract(concept, \"\\\\((.*?)\\\\)\"),\n   status = if_else(str_detect(label, 'above'),\n                    'Above Poverty Line',\n                    'Below Poverty Line'\n                    ),\n\n    .keep= 'none'\n  )\n\npoverty_data$race &lt;- gsub(\"[\\\\(\\\\)]\", \"\", trimws(poverty_data$race))\npoverty_data$race &lt;- str_to_title(poverty_data$race)\n\npoverty_final &lt;- get_acs(\n  geography = 'state',\n  variables = poverty_data |&gt; pull(variable),\n  year = 2020,\n  geometry = TRUE\n) |&gt; \n  left_join(poverty_data) |&gt;\n  select(-moe, -variable, -GEOID) |&gt;\n  group_by(race, gender, NAME) |&gt;\n  mutate(\n    gendered_total = sum(estimate[status == 'Above Poverty Line'], estimate[status == 'Below Poverty Line']),\n    proportion = (estimate[status == 'Above Poverty Line']/gendered_total)\n  ) |&gt;\n  filter(status != 'Below Poverty Line') |&gt;\n  ungroup() \n\nGetting data from the 2016-2020 5-year ACS\n\n\nDownloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.\n\n\nJoining with `by = join_by(variable)`\n\n### creating difference table\npoverty_diff &lt;- poverty_final |&gt;\n  select(-estimate, -status, -gendered_total)\n\npoverty_diff &lt;- as_tibble(poverty_diff)\n\npoverty_diff &lt;- poverty_diff %&gt;% select(-geometry)\n\npoverty_diff_table &lt;- poverty_diff |&gt;\n  group_by(race, NAME) |&gt;\n  mutate(`Percent Difference` = ((proportion[gender == 'Male'] - proportion[gender == 'Female'])*100)) |&gt;\n  filter(gender == 'Male') |&gt;\n  ungroup() |&gt;\n  select(-gender, -proportion) |&gt;\n  pivot_wider(names_from = race, values_from = `Percent Difference`)\n### This table shows the percent difference between the proportions of males who live above the poverty line and females, split by race and state. It is a way of showing the raw data of the differences we can observe on the heat maps between races.\n\n### creating plotted data  \npoverty_plot &lt;- poverty_final |&gt;\n  select(-estimate, -NAME, -status, -gendered_total) |&gt;\n  pivot_wider(names_from = c(gender,race), values_from = proportion, names_sep = ' ')\n  \ncolumn_names &lt;- names(poverty_plot)[-which(names(poverty_plot) == \"geometry\")]\n\nfor (x in column_names) {\n  p &lt;- ggplot(data = poverty_plot, aes(fill = .data[[x]])) +\n    geom_sf() +\n    coord_sf(crs = 5070) +\n    scale_fill_viridis_c(option = 'plasma', limits = c(0.3, 1)) +\n    theme_minimal() +\n    theme(legend.title = element_blank()) +\n    labs(title = x, caption = \"Source: ACS Poverty Status Data, Extracted with tidycensus.\")\n  \n  ggsave(paste0(x, \".jpg\"), plot = p, device = \"jpg\")\n}\n\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image\nSaving 7 x 5 in image"
  },
  {
    "objectID": "playing with data/ACS Cleaning.html",
    "href": "playing with data/ACS Cleaning.html",
    "title": "ACS Cleaning",
    "section": "",
    "text": "suppressMessages(library(readxl))\nsuppressMessages(library(tidyverse))\n\nacs_data &lt;- read.csv('dataset/ACS_data_rendered.csv')\n\nstate_names &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\", \"Puerto Rico\")\n\nrace_names &lt;- c(\"White Alone\", \"White alone, not Hispanic or Latino\", \"Black Alone\", \"American Indian or Alaska Native Alone\", \"Asian Alone\", \"Native Hawaiian and Other Pacific Islander Alone\", \"Some other race Alone\", \"Two or more races\", \"Hispanic or Latino Origin\")\n\n\ngendered_raced_data &lt;- function(race_names, state_names, data) {\n  race_data &lt;- list()\n  for (race in race_names) {\n    state_data &lt;- list()\n    for (index in 1:51) {\n      state = state_names[index]\n      multiplier &lt;- which(race_names == race) \n      string2 &lt;- rep(\"*\", multiplier)\n      label &lt;- paste(\"Bachelor's degree or higher\", paste(string2, collapse = \"\"), \n                     sep = \"\")\n      filtered_data &lt;- filter(acs_data, Label == label)\n      number_m_bach &lt;- 9 + 12 * (index - 1)\n      number_f_bach &lt;- 13 + 12 * (index - 1)\n      state_data[[state]] &lt;- tibble(\n        State = state,\n        Years = filtered_data$Years,\n        Male_P_Bach = filtered_data[[paste0(\"...\", number_m_bach)]],\n        Female_P_Bach = filtered_data[[paste0(\"...\", number_f_bach)]],\n        Race = race\n      )\n    }\n    combined_state &lt;- bind_rows(state_data)\n    race_data[[race]] &lt;- combined_state\n  }\n  # if you want separate tables for each race, comment out this line of data\n  race_data &lt;- bind_rows(race_data)\n  \n  return(race_data)\n}\n\n\ngendered_raced_data_num &lt;- function(race_names, state_names, data) {\n  race_data &lt;- list()\n  for (race in race_names) {\n    state_data &lt;- list()\n    for (index in 1:51) {\n      state = state_names[index]\n      multiplier &lt;- which(race_names == race) \n      string2 &lt;- rep(\"*\", multiplier)\n      label &lt;- paste(\"Bachelor's degree or higher\", paste(string2, collapse = \"\"), \n                     sep = \"\")\n      filtered_data &lt;- filter(acs_data, Label == label)\n      number_m_bach &lt;- 7 + 12 * (index - 1)\n      number_f_bach &lt;- 11 + 12 * (index - 1)\n      state_data[[state]] &lt;- tibble(\n        State = state,\n        Years = filtered_data$Years,\n        `Bachelor Attainment (Male)` = filtered_data[[paste0(\"...\", number_m_bach)]],\n        `Bachelor Attainment (Female)` = filtered_data[[paste0(\"...\", number_f_bach)]],\n        Race = race\n      )\n    }\n    combined_state &lt;- bind_rows(state_data)\n    race_data[[race]] &lt;- combined_state\n  }\n  # if you want separate tables for each race, comment out this line of data\n  race_data &lt;- bind_rows(race_data)\n  \n  return(race_data)\n}\n\n\ndata &lt;- gendered_raced_data(race_names, state_names, acs_data)\nwrite_csv(data, here::here('dataset', 'ACS_Gender_and_Race.csv'))\n\n\nnum_data &lt;- gendered_raced_data_num(race_names, state_names, acs_data)\n\nprepped_num &lt;- num_data|&gt;\n   mutate(`Bachelor Attainment (Male)` = as.numeric(gsub(\"[^0-9.]\", \"\", `Bachelor Attainment (Male)`)),\n         `Bachelor Attainment (Female)` = as.numeric(gsub(\"[^0-9.]\", \"\", `Bachelor Attainment (Female)`)))|&gt;\n  group_by(Years, Race) %&gt;%\n  summarise(`Total Male` = sum(`Bachelor Attainment (Male)`, na.rm = TRUE),\n            `Total Female` = sum(`Bachelor Attainment (Female)`, na.rm = TRUE)) #|&gt;\n\n`summarise()` has grouped output by 'Years'. You can override using the\n`.groups` argument.\n\n  #pivot_wider(names_from = Race, values_from = c(`Total Male`,`Total Female`), names_sep = ' ')\n\nprepped_num$Race &lt;- str_replace(prepped_num$Race, \"Native Hawaiian and Other Pacific Islander Alone\", \"NHPI\")\n\nprepped_num$Race &lt;- str_replace(prepped_num$Race, \"American Indian or Alaska Native Alone\", \"AIAN\")\n\nprepped_num$Race &lt;- str_replace(prepped_num$Race, \"White alone, not Hispanic or Latino\", 'White, Not Hispanic')\n\nwrite_csv(prepped_num, here::here('dataset', 'Numerical_ACS_Gender_and_Race.csv'))"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Data Page\n\nThis page serves as a gateway of the sources of our data and the cleaning process. To understand racial education division, we gathered three datasets, capturing variables like educational attainment, income levels, and poverty status across different demographics in the United States. Our data sources include the American Community Survey (ACS) and the U.S. Department of Labor (DOL).\nData set 1: American Community Survey (ACS) - Educational Attainment\nFinding data on the educational attainment and income of women was relatively easy, but specifically searching for those data faceted by a woman’s race was challenging.\nData set 1 is our main dataset. It is named “Educational Attainment” and sourced from the American Community Survey (ACS) produced by the United States Census Bureau which provides data from 2010-2022 regarding educational attainment of males and females, faceted to race and state. This data set also provides general information on the poverty rate for ages 25 by educational attainment and the inflation-adjusted yearly median earnings by educational attainment - although these additional data are not faceted by race and can only help as background/general information. The link to the ACS dataset is https://bit.ly/49WpvK9.\nThe ACS is specifically designed to assess various demographic, social, and economic conditions across the U.S. It helps measure the changes in the community over time. Gathering this data annually allows for the effective monitoring of trends and the ability to react to shifts in educational attainment, poverty rates, and other important socioeconomic factors. This survey plays a crucial role in public administration by guiding the allocation of federal and state funds, planning educational programs, and developing social services tailored to the needs of diverse populations.\nThe variables of dataset 1 include time in years, race, educational attainment, region, and gender. The time variable ranges from 2010 to 2022. The race variable consists of 9 categories: ‘White Alone’, ‘White Alone - not Hispanic or Latino’, ‘Black Alone’, ‘American Indian or Alaska Native Alone’, ‘Asian Alone’, ‘Native Hawaiian and Other Pacific Islander Alone’, ‘Some other race alone’, ‘Two or more races’, and ‘Hispanic or Latino Origin’. Educational attainment is broken down by ‘High school graduate or higher’ and ‘Bachelor’s degree or higher’, and is shown as a percentage of male and female through the gender variable. The regional variable contains the 50 states of the United States and Puerto Rico.\nTo clean dataset 1, we followed the following steps:\n1. Initial Data Reading and Package Loading\nWe begin by suppressing messages to keep our output clean while loading necessary libraries (‘readxl’ and ‘tidyverse’). This ensures that only essential output is displayed, making our script more readable.\n\nsuppressMessages(library(readxl))\nsuppressMessages(library(tidyverse))\n\nacs_data &lt;- read.csv('dataset/ACS_data_rendered.csv')\n\n2. Standardizing Reference Arrays\nWe define arrays for ‘state_names’ and ‘race_names’ to ensure consistency in how we reference these categories throughout our analysis. This is a form of data standardization, where we ensure that all data manipulation and analysis refer to the same set of standardized category names.\n\nstate_names &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\", \"Puerto Rico\")\n\nrace_names &lt;- c(\"White Alone\", \"White alone, not Hispanic or Latino\", \"Black Alone\", \"American Indian or Alaska Native Alone\", \"Asian Alone\", \"Native Hawaiian and Other Pacific Islander Alone\", \"Some other race Alone\", \"Two or more races\", \"Hispanic or Latino Origin\")\n\n3. Creating 2 functions to pull data from the ACS\nThe first function uses a for loop to loop through each races in ‘race_names’ for each of the races used a nested ‘for loop’ to go through each of the states in ‘state_names’. In the nested for loop, we identify which columns we need to pull from for the gendered percentage attainment of bachelor degrees and transfer these columns into new data tibbles (organized by state). We then combine these tables to put it into one table.\n\ngendered_raced_data &lt;- function(race_names, state_names, data) {\n  race_data &lt;- list()\n  for (race in race_names) {\n    state_data &lt;- list()\n    for (index in 1:51) {\n      state = state_names[index]\n      multiplier &lt;- which(race_names == race) \n      string2 &lt;- rep(\"*\", multiplier)\n      label &lt;- paste(\"Bachelor's degree or higher\", paste(string2, collapse = \"\"), \n                     sep = \"\")\n      filtered_data &lt;- filter(acs_data, Label == label)\n      number_m_bach &lt;- 9 + 12 * (index - 1)\n      number_f_bach &lt;- 13 + 12 * (index - 1)\n      state_data[[state]] &lt;- tibble(\n        State = state,\n        Years = filtered_data$Years,\n        Male_P_Bach = filtered_data[[paste0(\"...\", number_m_bach)]],\n        Female_P_Bach = filtered_data[[paste0(\"...\", number_f_bach)]],\n        Race = race\n      )\n    }\n    combined_state &lt;- bind_rows(state_data)\n    race_data[[race]] &lt;- combined_state\n  }\n  race_data &lt;- bind_rows(race_data)\n  \n  return(race_data)\n}\n\nThe second function works similarly to the first, except we do not have the outer for loop for the different races as the median income data is not categorized by race. We still loop through the different states and pull the columns containing the median incomes for people with bachelor degrees and people with graduate (or higher) degrees.\n\nMedian_Income_Data &lt;- function (state_names, data) {\n  state_data &lt;- list()\n  for (index in 1:51) {\n    state = state_names[index]\n    bach_data &lt;- filter(acs_data, Label == \"Bachelor's degree\")\n    grad_data &lt;- filter(acs_data, Label == \"Graduate or professional degree\")\n    male &lt;- 7 + 12 * (index - 1)\n    female &lt;- 11 + 12 * (index - 1)\n    both &lt;- state\n    state_data[[state]] &lt;- tibble(\n      State = state,\n      Years = bach_data$Years,\n      Median_Bach = bach_data[[both]],\n      Bach_M_Median = bach_data[[paste0(\"...\", male)]],\n      Bach_F_Median = bach_data[[paste0(\"...\", female)]],\n      Median_Grad = grad_data[[both]],\n      Grad_M_Median = grad_data[[paste0(\"...\", male)]],\n      Grad_F_Median = grad_data[[paste0(\"...\", female)]]\n    ) \n    \n  }\n  combined_state &lt;- bind_rows(state_data)\n  combined_state &lt;- combined_state %&gt;% \n    filter(Years &gt; 2014)\n  \n  return(combined_state)\n}\n\ndata2 &lt;- Median_Income_Data(state_names, acs_data)\nwrite_csv(data2, here::here('dataset', 'ACS_Median_Income.csv'))\n\n4. Calling the functions and writing new CSV’s\nWe call the functions to write 2 new dataframes. Then we write the data frames into new CSV’s to make it easily accessible without having to rerun the script.\n\ndata &lt;- gendered_raced_data(race_names, state_names, acs_data)\nwrite_csv(data, here::here('dataset', 'ACS_Gender_and_Race.csv'))\n\nThe cleaning script for dataset 1 can be found below:\n\ninvisible(source(\"scripts/acs_bach.R\")) \nscript_content &lt;- readLines(\"scripts/acs_bach.R\") \ncat(script_content, sep = \"\\n\")\n\n\n# Reading Data and Initializing Packages\nsuppressMessages(library(readxl))\nsuppressMessages(library(tidyverse))\n\nacs_data &lt;- read.csv('dataset/ACS_data_rendered.csv')\n\nstate_names &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\", \"Puerto Rico\")\n\nrace_names &lt;- c(\"White Alone\", \"White alone, not Hispanic or Latino\", \"Black Alone\", \"American Indian or Alaska Native Alone\", \"Asian Alone\", \"Native Hawaiian and Other Pacific Islander Alone\", \"Some other race Alone\", \"Two or more races\", \"Hispanic or Latino Origin\")\n\n# function to extract Bachelor Attainment Percentage\ngendered_raced_data &lt;- function(race_names, state_names, data) {\n  race_data &lt;- list()\n  for (race in race_names) {\n    state_data &lt;- list()\n    for (index in 1:51) {\n      state = state_names[index]\n      multiplier &lt;- which(race_names == race) \n      string2 &lt;- rep(\"*\", multiplier)\n      label &lt;- paste(\"Bachelor's degree or higher\", paste(string2, collapse = \"\"), \n                     sep = \"\")\n      filtered_data &lt;- filter(acs_data, Label == label)\n      number_m_bach &lt;- 9 + 12 * (index - 1)\n      number_f_bach &lt;- 13 + 12 * (index - 1)\n      state_data[[state]] &lt;- tibble(\n        State = state,\n        Years = filtered_data$Years,\n        Male_P_Bach = filtered_data[[paste0(\"...\", number_m_bach)]],\n        Female_P_Bach = filtered_data[[paste0(\"...\", number_f_bach)]],\n        Race = race\n      )\n    }\n    combined_state &lt;- bind_rows(state_data)\n    race_data[[race]] &lt;- combined_state\n  }\n  # if you want separate tables for each race, comment out this line of data\n  race_data &lt;- bind_rows(race_data)\n  \n  return(race_data)\n}\n\nMedian_Income_Data &lt;- function (state_names, data) {\n  state_data &lt;- list()\n  for (index in 1:51) {\n    state = state_names[index]\n    bach_data &lt;- filter(acs_data, Label == \"Bachelor's degree\")\n    grad_data &lt;- filter(acs_data, Label == \"Graduate or professional degree\")\n    male &lt;- 7 + 12 * (index - 1)\n    female &lt;- 11 + 12 * (index - 1)\n    both &lt;- state\n    state_data[[state]] &lt;- tibble(\n      State = state,\n      Years = bach_data$Years,\n      Median_Bach = bach_data[[both]],\n      Bach_M_Median = bach_data[[paste0(\"...\", male)]],\n      Bach_F_Median = bach_data[[paste0(\"...\", female)]],\n      Median_Grad = grad_data[[both]],\n      Grad_M_Median = grad_data[[paste0(\"...\", male)]],\n      Grad_F_Median = grad_data[[paste0(\"...\", female)]]\n    ) \n    \n  }\n  combined_state &lt;- bind_rows(state_data)\n  combined_state &lt;- combined_state %&gt;% \n    filter(Years &gt; 2014)\n  \n  return(combined_state)\n}\n\nMedian_Income_Data_Perc &lt;- function (state_names, data) {\n  state_data &lt;- list()\n  for (index in 1:51) {\n    state = state_names[index]\n    bach_data &lt;- filter(acs_data, Label == \"Bachelor's degree\")\n    grad_data &lt;- filter(acs_data, Label == \"Graduate or professional degree\")\n    male &lt;- 9 + 12 * (index - 1)\n    female &lt;- 13 + 12 * (index - 1)\n    both &lt;- state\n    state_data[[state]] &lt;- tibble(\n      State = state,\n      Years = bach_data$Years,\n      Median_Bach = bach_data[[both]],\n      Bach_M_Median = bach_data[[paste0(\"...\", male)]],\n      Bach_F_Median = bach_data[[paste0(\"...\", female)]],\n      Median_Grad = grad_data[[both]],\n      Grad_M_Median = grad_data[[paste0(\"...\", male)]],\n      Grad_F_Median = grad_data[[paste0(\"...\", female)]]\n    ) \n    \n  }\n  combined_state &lt;- bind_rows(state_data)\n  combined_state &lt;- combined_state %&gt;% \n    filter(Years &gt; 2014)\n  \n  return(combined_state)\n}\n\ndata2 &lt;- Median_Income_Data(state_names, acs_data)\nwrite_csv(data2, here::here('dataset', 'ACS_Median_Income.csv'))\n\ndata3 &lt;- Median_Income_Data_Perc(state_names, acs_data)\nwrite.csv(data3, here::here('dataset', 'ACS_Median_Income_Perc.csv'))\n\n# Save Data\ndata &lt;- gendered_raced_data(race_names, state_names, acs_data)\nwrite_csv(data, here::here('dataset', 'ACS_Gender_and_Race.csv'))\n\n\nData set 2: U.S. Department of Labor (DOL) Data - Median Annual Earnings\nOur secondary dataset is from the U.S. Department of Labor (DOL), specifically through the Bureau of Labor Statistics (BLS). The data set is named “Median annual earnings by sex, race, and Hispanic ethnicity”, providing data from 1960 - 2022 regarding the median average earnings of males and females, faceted to races and states. The U.S. Bureau of Labor Statistics collects, analyzes, and publishes reliable information on socioeconomic status. The link to this DOL dataset is https://bit.ly/3JAw0qz.\nThe data from the DOL, specifically the median annual earnings by sex, race, and Hispanic ethnicity, are collected to track economic trends and labor market conditions. This information is critical for formulating labor policies, understanding income disparities, and enforcing laws around wages and employment equity\nThe dependent variable in this data is median annual earnings in 2021 CPI-U-RS adjusted dollars. The independent variable is the years from 1960 to 2023. And, the data is filled with the following gender and race groups: White women, White men, Black women, Black men, Asian women, Asian men, Hispanic women, Hispanic men, White non-Hispanic Women, White non-Hispanic Men.\nThe data cleaning process for dataset 2 is below:\n1. Initial Data Reading and Package Loading\nWe loaded the data using ‘read_csv’ from the ‘readr’ package. This function is efficient for handling large datasets and allows easier manipulation of our dataset.\n\nincome_data = read_csv(\"dataset/Income_dofl.csv\", show_col_types = FALSE)\n\n2. Filtering Data Based on Specific Criteria\nWe filtered out records from 2013 and 2017 unless they were part of a new methodology implementation. In this way, we can ensure consistency in how data across different years were collected and reported.\n\nincome_data_clean &lt;- income_data|&gt;\n  filter(Year != 2013 | `In / Out of New Methodology Implemented` == 'In')|&gt;\n  filter(Year != 2017 | `In / Out of New Methodology Implemented` == 'In')\n\n3. Selecting and Pivoting Data for Analysis\nWe transformed the dataset from a long to a wide format using ‘pivot_wider’, making it easier to analyze income measures across different years. Columns were arranged by year to maintain a chronological order.\n\nincome_data_clean &lt;- income_data_clean |&gt;\n  select(-c(1,2)) |&gt; \n  group_by(`Measure Names`) |&gt;\n  pivot_wider(names_from = `Measure Names`, values_from = `Measure Values`) |&gt;\n  arrange(Year)\n\n4. Saving Cleaned Data\n\nwrite_csv(income_data_clean, file = here::here('dataset', 'dofl_income_clean.csv'))\nsaveRDS(income_data_clean, file = here::here('dataset/dofl_income_clean.RData'))\n\nThe cleaning script for dataset 2 can be found below:\n\ninvisible(source(\"scripts/Income_data_cleaning.R\"))\n\nRows: 780 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): In / Out of New Methodology Implemented, In / Out of New Processing...\ndbl (1): Year\nnum (1): Measure Values\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nscript_content &lt;- readLines(\"scripts/Income_data_cleaning.R\")\n\nWarning in readLines(\"scripts/Income_data_cleaning.R\"): incomplete final line\nfound on 'scripts/Income_data_cleaning.R'\n\ncat(script_content, sep = \"\\n\")\n\nincome_data = read_csv(\"dataset/Income_dofl.csv\")\n### median income data split by race\n\nincome_data_clean &lt;- income_data|&gt;\n  filter(Year != 2013 | `In / Out of New Methodology Implemented` == 'In')|&gt;\n  filter(Year != 2017 | `In / Out of New Methodology Implemented` == 'In')|&gt;\n  select(-c(1,2)) |&gt;\n  group_by(`Measure Names`) |&gt;\n  pivot_wider(names_from = `Measure Names`, values_from = `Measure Values`) |&gt; \n  arrange(Year)\n\nwrite_csv(income_data_clean, file = here::here('dataset', 'dofl_income_clean.csv'))\nsaveRDS(income_data_clean, file = here::here('dataset/dofl_income_clean.RData'))\n\n\nData set 3: American Community Survey (ACS) - Poverty Status\nAnother dataset we used is from the American Community Survey (ACS), which provides data from 2015 - 2020 regarding poverty status of males, and females, faceted to race and state. We selected this dataset because we wanted to analyze gender disparities, particularly seeing the proportion of males above poverty line compared to females, by race and state.\nThe dataset is accessed via the ‘tidycensus’ package in R because the ‘tidycensus’ package simplifies the process of working with Census data, allowing for efficient data retrieval and handling. To load the variables for the year 2020, we use the function ‘load_variables(2020, “acs5”, cache = TRUE)’. This function fetches a list of available variables from the 5-year ACS dataset for 2020, caching them to speed up future queries.\nThe dependent variable in this dataset is the poverty status, which is defined as the proportion of individuals below the poverty line in each surveyed group. The independent variables include the year of the survey, which ranges from 2015-2020, and demographic identifiers such as race and state."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n\n### importing data for first models\nacs_income &lt;- read_csv(here::here('dataset/ACS_Median_Income.csv'))\n\nRows: 408 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): State\ndbl (1): Years\nnum (6): Median_Bach, Bach_M_Median, Bach_F_Median, Median_Grad, Grad_M_Medi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n###Summarize Income data\nacs_income_mean &lt;- acs_income |&gt;\n dplyr::select(Years, Bach_M_Median, Bach_F_Median, Median_Bach)|&gt;\n  group_by(Years)|&gt;\n  summarise(\n    `Average Median Income Bachelors (Male)` = mean(Bach_M_Median),\n    `Average Median Income Bachelors (Female)` = mean(Bach_F_Median),\n    `Average Median Income Bachelors` = mean(Median_Bach, na.rm = T)\n  )\n\n### preliminary model of income data checking (first need to see distribution)\nggplot(data = acs_income_mean, aes(x = Years)) + geom_line(aes(y = `Average Median Income Bachelors`, color = \"total\")) + geom_line(aes(y = `Average Median Income Bachelors (Male)`, color = 'male')) + geom_line(aes(y = `Average Median Income Bachelors (Female)`, color = 'female')) + scale_color_manual(values = c('total' = 'black', 'male' = 'blue', 'female' = 'red')) + theme_minimal()\n\n\n\n### looking at this graph, it is clear to see the data in general follows a linear trend. I have decided to use a VAR model. We can also see that the changes in each group by year are consistent\n\n### model 1 (VAR) of AVG. Median Income\n\nmodel1 &lt;- VAR(acs_income_mean[, -1], p = 1)\n\ntidied &lt;- broom::tidy(model1)\n\ntable1 &lt;- tidied |&gt;\n  filter(group == 'Average.Median.Income.Bachelors') \n\nknitr::kable(table1)\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nAverage.Median.Income.Bachelors\nAverage.Median.Income.Bachelors..Male..l1\n2.826356\n2.312574\n1.2221692\n0.3089107\n\n\nAverage.Median.Income.Bachelors\nAverage.Median.Income.Bachelors..Female..l1\n-4.521218\n1.912971\n-2.3634530\n0.0990894\n\n\nAverage.Median.Income.Bachelors\nAverage.Median.Income.Bachelors.l1\n2.445225\n3.106696\n0.7870822\n0.4886858\n\n\nAverage.Median.Income.Bachelors\nconst\n-40792.764381\n13745.460492\n-2.9677263\n0.0591759\n\n\n\n\n### model 2 ARIMA of each individual variable\n\nar1 &lt;- arima(acs_income_mean$`Average Median Income Bachelors`, order = c(1,1,1))\n\nar1_t &lt;- broom::tidy(ar1)\n\nar2 &lt;- arima(acs_income_mean$`Average Median Income Bachelors (Male)`, order = c(1,1,1))\n\nar2_t &lt;- broom::tidy(ar2)\n\nar3 &lt;- arima(acs_income_mean$`Average Median Income Bachelors (Female)`, order = c(1,1,1)) \n\nar3_t &lt;- broom::tidy(ar3)\n\ncombined_table &lt;- bind_rows(\n  ar1_t %&gt;% mutate(variable = \"Average Median Income Bachelors\"),\n  ar2_t %&gt;% mutate(variable = \"Average Median Income Bachelors (Male)\"),\n  ar3_t %&gt;% mutate(variable = \"Average Median Income Bachelors (Female)\")\n) |&gt; filter(term == 'ar1')\n  \n\n# Print the combined table\nknitr::kable(combined_table)\n\n\n\n\nterm\nestimate\nstd.error\nvariable\n\n\n\n\nar1\n0.8774323\n0.2161996\nAverage Median Income Bachelors\n\n\nar1\n0.8432083\n0.2750058\nAverage Median Income Bachelors (Male)\n\n\nar1\n0.8997792\n0.2173128\nAverage Median Income Bachelors (Female)\n\n\n\n\n### This is the set of the auto regressive coefficients of each of these variables analysed through a time series model. We can find that their coefficients are extremely similar, implying that our variables all share similar levels of autocorrelation, leading them to all have similar momentum, which implies that the pay gap between men and women is relatively unchanged\n\n\nbachelors_attinment &lt;- read_csv(here::here('dataset/Numerical_ACS_Gender_and_Race.csv'))\n\nRows: 72 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Race\ndbl (3): Years, Total Male, Total Female\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndofl &lt;- read_csv(here::here('dataset/dofl_income_clean.csv'))\n\nRows: 63 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (13): Year, Total Women, Total Men, White Women, White Men, Black Women,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndofl_prep &lt;- dofl|&gt;\n  pivot_longer(cols = -Year, names_to = 'Race', values_to = 'Income') |&gt;\n  mutate(Gender = str_replace(str_replace(str_extract(Race, \"Men|Women\"), \"Men\", \"Male\"), \"Women\", \"Female\"))\n\nggplot(data = dofl_prep, aes(x = Year, y = Income, color = Race)) +\n  geom_line() +\n  labs(x = \"Years\", y = \"Income\", color = \"Race\") +\n  theme_minimal()\n\nWarning: Removed 166 rows containing missing values (`geom_line()`).\n\n\n\n\n### Above is a line graph of income data for each race and gender\n\n\n## merging attainment data with income\ndofl_merge &lt;- dofl_prep |&gt;\n  mutate(Race = str_replace_all(Race, c(\" Men\" = \"\", \" Women\" = \"\"))) |&gt;\n  filter(Year &gt;= 2015) |&gt;\n  filter(Race != 'Total') |&gt;\n  arrange(Race)\n\ndesired_races &lt;- c(\"White\", \"Black\", \"Hispanic\", \"White, non-Hispanic\", \"Asian\")\n\nbachelor_merge &lt;- bachelors_attinment |&gt;\n  mutate(Race = str_replace_all(Race, c(\" Alone\" = '', \"Not Hispanic\"= 'non-Hispanic',' or Latino Origin' = ''))) |&gt;\n  filter(Race %in% desired_races) |&gt;\n  pivot_longer(cols = c(\"Total Male\", \"Total Female\"), \n               names_to = \"Gender\",\n               values_to = \"Bachelors Attainment\") |&gt;\n  mutate(Gender = str_replace_all(Gender, c('Total ' = ''))) |&gt;\n  arrange(Race)\n\nmodeling_data &lt;- dofl_merge |&gt;\n  mutate(Bachelors_Attainment = bachelor_merge$`Bachelors Attainment`)\n\n\nTo be or not to be. ```\n\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team 8. The members of this team are below."
  },
  {
    "objectID": "about.html#zixinalicezhang",
    "href": "about.html#zixinalicezhang",
    "title": "About",
    "section": "Zixin(Alice)Zhang",
    "text": "Zixin(Alice)Zhang\nAlice is a senior student major in Economics."
  },
  {
    "objectID": "about.html#dylon-thompson",
    "href": "about.html#dylon-thompson",
    "title": "About",
    "section": "Dylon Thompson",
    "text": "Dylon Thompson\nDylon is a senior student majoring in Economics and Mathematics."
  },
  {
    "objectID": "about.html#bhimraj-singh-bhuller",
    "href": "about.html#bhimraj-singh-bhuller",
    "title": "About",
    "section": "Bhimraj Singh Bhuller",
    "text": "Bhimraj Singh Bhuller\n\n\n\n\n\nLinkedIn: linkedin.com/in/bhimrajbhuller\nBhimraj is a sophomore pursuing a dual degree program in B.A. in Economics and Mathematics and M.A. in Economics. He finds joy in using knowledge to enhance lives and processes. Economics is his playground, and he is an overenthusiastic problem-crusher. When not decoding economic mysteries, he is on a book binge or catching up with friends and family.\nIn the economics arena, he is heads over heels for macroeconomics. Reading financial crisis books is a passion, giving him a lowdown on economic rollercoasters. Beyond that, he kicks a ball around, and travel to broaden his perspectives."
  },
  {
    "objectID": "about.html#tianxiang-chen",
    "href": "about.html#tianxiang-chen",
    "title": "About",
    "section": "Tianxiang Chen",
    "text": "Tianxiang Chen\nTianxiang is a graduate student major in MA Economics."
  },
  {
    "objectID": "about.html#gabe-gan",
    "href": "about.html#gabe-gan",
    "title": "About",
    "section": "Gabe Gan",
    "text": "Gabe Gan\nGabe is a senior student majoring in Applied Mathematics.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "Image: Demonstrators carry signs during the 2020 Women’s March on Jan. 18 in Washington, D.C. (Zach Gibson/Getty Images)"
  },
  {
    "objectID": "big_picture.html#charting-educational-journeys",
    "href": "big_picture.html#charting-educational-journeys",
    "title": "Big Picture",
    "section": "Charting Educational Journeys",
    "text": "Charting Educational Journeys\nThe project begins by mapping the educational voyage of women and men across the fifty states and Puerto Rico. The project then dissects the percentage of women that has attained a bachelor’s degree or higher, faceted by race. Through a lens of seven years, from 2015 - 2022, we project the percentage of women that has attained a bachelor’s degree of higher for the next five years. Yet, the journey does not end there; the project mirrors this process for men, dissecting the educational diversity with equal rigor.\n\n\nWarning: package 'shiny' was built under R version 4.3.3"
  },
  {
    "objectID": "big_picture.html#illuminating-the-gender-pay-gap",
    "href": "big_picture.html#illuminating-the-gender-pay-gap",
    "title": "Big Picture",
    "section": "Illuminating the Gender Pay Gap",
    "text": "Illuminating the Gender Pay Gap\nAlso guided by the beacon of economic insight, the project delves into income data of genders and races spanning sixty two years. The income data by the U.S. Department of Labor is more than statistics; they embody the struggle for equity. With each statistical model, this project sheds light on the pay gap differences between men and women, and between races.\n\n\n\n\n\nSource: Cleaned Dept. of Labor Data"
  },
  {
    "objectID": "big_picture.html#gender-and-race-dynamics-above-the-poverty-line",
    "href": "big_picture.html#gender-and-race-dynamics-above-the-poverty-line",
    "title": "Big Picture",
    "section": "Gender and Race Dynamics Above the Poverty Line",
    "text": "Gender and Race Dynamics Above the Poverty Line\nThis project also investigates the amount of females and males above the poverty line across U.S. states and sees the differences for different gender and races. The disparities are particularly seen for Native Hawaiian and Other Pacific Islander Alone, Black Alone, and most notably, American Indian and Alaska Native Alone populations. These findings further highlight the systemic injustices that contribute to inequality within communities."
  },
  {
    "objectID": "big_picture.html#not-done-yet-this-part-simplify-how-we-say-linear-regression.-the-results-are-not-yet-done-so-include-paragraph-parts-on-where-you-could-put-results",
    "href": "big_picture.html#not-done-yet-this-part-simplify-how-we-say-linear-regression.-the-results-are-not-yet-done-so-include-paragraph-parts-on-where-you-could-put-results",
    "title": "Big Picture",
    "section": "[NOT DONE YET THIS PART: simplify how we say linear regression. The results are not yet done (so include paragraph parts on where you could put results)]",
    "text": "[NOT DONE YET THIS PART: simplify how we say linear regression. The results are not yet done (so include paragraph parts on where you could put results)]\nThis project stands on the crossroads of data and destiny. Join us as we continue this quest for a more egalitarian society, one insight at a time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBlog Post 7\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBlog Post 3\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBlog Post 4\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBlog Post 5\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nBlog Post 6\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 1\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "playing with data/Approach.html",
    "href": "playing with data/Approach.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "A 2-step Approach (‘a model within a model’):\n\nWe find the relationship per State (via a linear model) to test the relationship of the percentage of a certain educational attainment to the median earnings of females, according to the certain educational attainment over 2010 to 2022.\n\nPredictor: Percentage of educational attainment Response: Median Earnings\n\nWe project the educational attainment of females per State according to their race, using past data from 2010 to 2022, by creating a linear model.\n\nPredictor: Time Response: Percentage of Educational attainment\n\nAfter creating a linear model in Q2, we can get a prediction of educational attainment over the next 25 years. Then, we use this prediction to our first linear model, to project the median earnings of each race from each State.\n\nCombine\n\nDo question 2 first, then do question 1.\n\nDivide:\nTianxiang - the first 25 states alphabetically Bhim - the rest\nStep by step. Then, reconvene in class.\nTo make it fancy:\nCheck the assumptions of linear regression after."
  },
  {
    "objectID": "playing with data/Organizied ACS Data.html",
    "href": "playing with data/Organizied ACS Data.html",
    "title": "Potential Graphs",
    "section": "",
    "text": "suppressMessages(library(readxl))\nsuppressMessages(library(tidyverse))\n\nacs_stuff &lt;- read.csv('dataset/ACS_Gender_and_Race.csv')\nincome_data &lt;- read.csv('dataset/dofl_income_clean.csv')\n\n\n# numeric for percentage\nacs_stuff$Male_P_Bach &lt;- as.numeric(gsub(\"%\", \"\", acs_stuff$Male_P_Bach))\n\nWarning: NAs introduced by coercion\n\nacs_stuff$Female_P_Bach &lt;- as.numeric(gsub(\"%\", \"\", acs_stuff$Female_P_Bach))\n\nWarning: NAs introduced by coercion\n\n# organizing raced percentage data\nacs_stuff %&gt;% group_by(Race) %&gt;%\n  summarize(\n    bach_perc_male = sum(Male_P_Bach, na.rm = TRUE) / n(),\n    bach_perc_female = sum(Female_P_Bach, na.rm = TRUE) / n(),\n    prop_m_to_f = bach_perc_female/bach_perc_male\n  )\n\n# A tibble: 9 × 4\n  Race                               bach_perc_male bach_perc_female prop_m_to_f\n  &lt;chr&gt;                                       &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt;\n1 American Indian or Alaska Native …           13.8             17.2       1.25 \n2 Asian Alone                                  56.6             50.5       0.892\n3 Black Alone                                  20.8             24.8       1.19 \n4 Hispanic or Latino Origin                    17.4             21.2       1.22 \n5 Native Hawaiian and Other Pacific…           11.2             11.9       1.07 \n6 Some other race Alone                        13.8             16.9       1.23 \n7 Two or more races                            27.7             32.1       1.16 \n8 White Alone                                  34.3             36.0       1.05 \n9 White alone, not Hispanic or Lati…           35.8             37.2       1.04 \n\n\n\n# numeric for median data\nview(income_data)\n\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the data\ndata &lt;- read_csv('dataset/ACS_Gender_and_Race.csv')\n\nRows: 3672 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): State, Male_P_Bach, Female_P_Bach, Race\ndbl (1): Years\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Select a specific state for the analysis\nselected_state &lt;- \"California\"\ndata_filtered &lt;- filter(data, State == selected_state)\n\n# Create the plot\nplot &lt;- ggplot(data_filtered, aes(x = Race, y = Male_P_Bach, fill = \"Male\")) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  geom_bar(aes(y = Female_P_Bach, fill = \"Female\"), stat = \"identity\", position = position_dodge()) +\n  labs(title = paste(\"Bachelor's Degree Attainment by Gender in\", selected_state),\n       x = \"Race\",\n       y = \"Percentage (%)\",\n       fill = \"Gender\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Print the plot\nprint(plot)\n\n\n\n\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Load the data\ndata &lt;- read_csv('dataset/ACS_Gender_and_Race.csv')\n\nRows: 3672 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): State, Male_P_Bach, Female_P_Bach, Race\ndbl (1): Years\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter data for the year 2022\ndata_filtered &lt;- filter(data, Years == 2022)\n\n# Create the bar plot\nplot &lt;- ggplot(data_filtered, aes(x = Race, y = Male_P_Bach, fill = \"Male\")) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9)) +\n  geom_bar(aes(y = Female_P_Bach, fill = \"Female\"), stat = \"identity\", position = position_dodge(width = 0.9)) +\n  scale_fill_manual(values = c(\"Male\" = \"blue\", \"Female\" = \"red\")) +\n  labs(title = \"Bachelor's Degree Attainment by Gender Across Races for 2022\",\n       x = \"Race\",\n       y = \"Percentage (%)\",\n       fill = \"Gender\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Print the plot\nprint(plot)\n\n\n\n\n\n# Load necessary libraries\nlibrary(tidyverse)\n\n# Read the data\ndata &lt;- read_csv('dataset/ACS_Gender_and_Race.csv')\n\nRows: 3672 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): State, Male_P_Bach, Female_P_Bach, Race\ndbl (1): Years\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Convert percentage columns to numeric\ndata$Male_P_Bach &lt;- as.numeric(gsub(\"%\", \"\", data$Male_P_Bach))\n\nWarning: NAs introduced by coercion\n\ndata$Female_P_Bach &lt;- as.numeric(gsub(\"%\", \"\", data$Female_P_Bach))\n\nWarning: NAs introduced by coercion\n\n# Filter data for the year 2022\ndata_filtered &lt;- filter(data, Years == 2022)\n\n# Create the bar plot\nplot &lt;- ggplot(data_filtered, aes(x = Race, fill = Gender)) +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9), aes(y = Male_P_Bach), fill = \"blue\") +\n  geom_bar(stat = \"identity\", position = position_dodge(width = 0.9), aes(y = Female_P_Bach), fill = \"red\") +\n  scale_y_continuous(breaks = seq(20, 100, by = 20), labels = scales::percent_format()) +\n  labs(title = \"Bachelor's Degree Attainment by Gender Across Races for 2022\",\n       x = \"Race\",\n       y = \"Percentage (%)\",\n       fill = \"Gender\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Print the plot\nprint(plot)\n\nWarning: Removed 31 rows containing missing values (`geom_bar()`).\n\n\nWarning: Removed 31 rows containing missing values (`geom_bar()`)."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-02-23-blog-post-1/blog-post-1.html",
    "href": "posts/2024-02-23-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "The purpose of this blog post is to highlight three potential data sets that our group may focus on for our final project."
  },
  {
    "objectID": "posts/2024-02-23-blog-post-1/blog-post-1.html#data-set-1-death-rates-for-suicide-by-race-age-and-hispanic-origin-in-the-united-states",
    "href": "posts/2024-02-23-blog-post-1/blog-post-1.html#data-set-1-death-rates-for-suicide-by-race-age-and-hispanic-origin-in-the-united-states",
    "title": "Blog Post 1",
    "section": "Data Set #1: Death rates for suicide by race, age, and Hispanic origin in the United States",
    "text": "Data Set #1: Death rates for suicide by race, age, and Hispanic origin in the United States\nThis data set was gathered from data.gov, a free database provided by the U.S. Government (https://catalog.data.gov/dataset/death-rates-for-suicide-by-sex-race-hispanic-origin-and-age-united-states-020c1). This data set has 6390 objects and 13 columns, and describes U.S. suicide rates from the years 1950 to 2018. Data before 1980 is a decade apart (1950, 1960, 1970, 1980) but yearly after 1980.\nThere is a large variety of groups that this data set is separated by, including aggregate data, gender, race, and age. This data was collected by the National Center for Health Statistics, and is highly categorized to the point where grabbing specific information from it has potential to be easy, but proper sorting may prove to be a challenge. Cleaning of the data appears to be unnecessary. While this topic is quite bleak, it could provide valuable insight into the well being of many demographics in the United States."
  },
  {
    "objectID": "posts/2024-02-23-blog-post-1/blog-post-1.html#data-set-2-usa-unemployment-rates-by-demographics-race",
    "href": "posts/2024-02-23-blog-post-1/blog-post-1.html#data-set-2-usa-unemployment-rates-by-demographics-race",
    "title": "Blog Post 1",
    "section": "Data Set #2: USA Unemployment Rates by Demographics & Race",
    "text": "Data Set #2: USA Unemployment Rates by Demographics & Race\nThe data set is from Kaggle: https://www.kaggle.com/datasets/asaniczka/unemployment-rates-by-demographics-1978-2023?resource=download This data set from Kaggle titled “Unemployment Rates by Demographics 1978-2023” contains data related to unemployment rates segmented by various demographic factors in the U.S. over a period spanning from 1978 to 2023. The data set includes 122 columns, including information of unemployment rates for various age groups, education levels, genders, races, and more. The data set includes 537 rows, which the frequency of data collecting is monthly from December 1978 to July 2023.\nThe data is sourced from the Economic Policy Institute’s State of Working America Data Library and economic research conducted by the Federal Reserve Bank of St. Louis. The purpose is to monitor and analyze employment trends in the population. I think we are able to load/clean this data because the segmentation is clear in columns so it is easy for us to screen out the exact data columns we are looking for for our main research focus and topic. There is no N/A in this data set. The main questions I hope to address with this dataset is to find relationship between education level and unemployment rate and find the trend for different demographic groups across time span."
  },
  {
    "objectID": "posts/2024-02-23-blog-post-1/blog-post-1.html#data-set-3-educational-attainment-of-genders-and-race-groups-across-various-age-groups-us-census-bureau",
    "href": "posts/2024-02-23-blog-post-1/blog-post-1.html#data-set-3-educational-attainment-of-genders-and-race-groups-across-various-age-groups-us-census-bureau",
    "title": "Blog Post 1",
    "section": "Data Set #3: Educational Attainment of Genders and Race Groups across various age groups (US Census Bureau)",
    "text": "Data Set #3: Educational Attainment of Genders and Race Groups across various age groups (US Census Bureau)\nThe data set is from the United States Census Bureau as part of the Bureau’s ‘American Community Survey’. The data set can be accessed by the following link: https://bit.ly/49WpvK9. This data set comprises of educational attainment of different age groups, the educational attainment of different race categories, the poverty rate for ages 25 by educational attainment level, and the inflation-adjusted yearly median earnings by race and educational attainment. We selected this data set as it contains a wealth of useful information, and ultimately contains figures for those stated variables for females (also comes complete with percentages also!). In addition, the data also includes specific margin of errors (at 90% margin of error) for each data point it includes - hence boosting its reliability.\nThe data set ranges from 2010 to 2022. A challenge is that the data set does not put all the data across 2010 - 2022 in one file (i.e., it separates them for observations for each year, resulting in 12 separate files). As of now, we have tried merging them into one file to ease reading. Nonetheless, we acknowledge that the data is not tidy and extensive data tidying is needed to process the data into the format we want. Technically, each separate file consists of 71 rows and 13 columns. There are also some parts of the data which are not available (i.e., denoted by ‘(X)’) - although only a negligible amount.\nUltimately, we conclude that this data set could prove extremely useful if we are able to ‘tidy’ it well.\n(Further information: For more supporting documentation on the accuracy of this data set (e.g., subject definitions, data accuracy, and statistical testing, it can be found on the Technical Documentation of the American Community Survey website in the following link: https://bit.ly/3SVaf9f)"
  },
  {
    "objectID": "posts/blog-post-3/blogpost3.html",
    "href": "posts/blog-post-3/blogpost3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "Blogpost 3 (for 04/01)\nFor this week, we obtained methods from the Professor on how to extract data from an ACS file, particularly about the tidycensus package (link here: https://bit.ly/43GG3Uc) . This is particularly useful as our main data set is from the ACS. \nAdditionally, we also cleaned our Gender Data form the World Bank by isolating the indicator name and codes relatively easily. After, we briefly explored the data by making charts to see how indicators relate with each other (e.g., for instance, we noticed something interesting about converging employment-to-population ratios for men and women). Additionally, we also cleaned another dataset on death rates by suicide and analyzed the death rates for females from different races over the years. Interestingly, the death rates by suicide across all races stayed constant over the years (we are not sure if we made a mistake, if the data is unreliable, or actually reflects the truth). The R code chunk when analyzing the death rate data is below: \n\nlibrary(ggplot2)\nlibrary(readxl)\n\ndata &lt;- read_excel(\"dataset/Death rates by suicide.xlsx\")\n\n##For Female: White category\n\nfiltered_data &lt;- subset(data, STUB_LABEL == \"Female: White\" & STUB_NAME == \"Sex and race\")\n\nggplot(filtered_data, aes(x = YEAR, y = STUB_LABEL_NUM)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  labs(title = \"Death Rates by Suicide for Female: White\",\n       x = \"Year\",\n       y = \"Death Rates per 100,000 people\",\n       caption = \"Data source: To be filled\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 10),\n        axis.text = element_text(size = 10),\n        legend.position = \"none\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n##For Female: Black or African category\n\nfiltered_data &lt;- subset(data, STUB_LABEL == \"Female: Black or African American\" & STUB_NAME == \"Sex and race\")\n\nggplot(filtered_data, aes(x = YEAR, y = STUB_LABEL_NUM)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  labs(title = \"Death Rates by Suicide for Female: Black or African American\",\n       x = \"Year\",\n       y = \"Death Rates per 100,000 people\",\n       caption = \"Data source: Death_rates_by_suicide dataset\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12),\n        legend.position = \"none\")\n\n\n\n## For Female: Female: American Indian or Alaska Native\n\nfiltered_data &lt;- subset(data, STUB_LABEL == \"Female: American Indian or Alaska Native\" & STUB_NAME == \"Sex and race\")\n\nggplot(filtered_data, aes(x = YEAR, y = STUB_LABEL_NUM)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  labs(title = \"Death Rates by Suicide for Female: American Indian or Alaska Native\",\n       x = \"Year\",\n       y = \"Death Rates per 100,000 people\",\n       caption = \"Data source: Death_rates_by_suicide dataset\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12),\n        legend.position = \"none\")\n\n\n\n##For Female: Asian or Pacific Islander\n\nfiltered_data &lt;- subset(data, STUB_LABEL == \"Female: Asian or Pacific Islander\" & STUB_NAME == \"Sex and race\")\n\nggplot(filtered_data, aes(x = YEAR, y = STUB_LABEL_NUM)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  labs(title = \"Death Rates by Suicide for Female: Asian or Pacific Islander\",\n       x = \"Year\",\n       y = \"Death Rates per 100,000 people\",\n       caption = \"Data source: Death_rates_by_suicide dataset\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 16, face = \"bold\"),\n        axis.title = element_text(size = 14),\n        axis.text = element_text(size = 12),\n        legend.position = \"none\")\n\n\n\n\n{Describe and/or put more data analysis attempts here, if any}\n\nOn the other hand, we also used this week to review linear regression methodologies, particularly the methods to check the assumptions of the linear model (i.e., Residuals vs. Fitted plots, Q-Q plots, Scale-Location plot (very similar to Residuals vs. Fitted plots), and Residuals vs Leverage (Cook’s Distance) plots). We will use these extensively in our project to see if our data posits linear patterns, and rely on the Residuals vs. Leverage method to remove outliers and boost accuracy of our model. Throughout the project, we also want to experiment with using more predictor variables for a certain response variable to see if it will increase accuracy. \nSome challenges were that we were met with difficulties using the tidycensus package. Even though we have inserted the correct API key provided by the Census Bureau, an error persistently occurred. We will consult with the Professor to uncover this bug. Due to this, we were not able to analyze our ACS data extensively."
  },
  {
    "objectID": "posts/blog-post-5/blog-post-5.html",
    "href": "posts/blog-post-5/blog-post-5.html",
    "title": "Team 8 Final Project",
    "section": "",
    "text": "Blog Post 5\n04/15/2024\nWe are planning on combining data from three sources: The World Bank, ACS data supplied by the census bureau, and data from the 2020 and 2010 census as well. Our goal is to paint a complete picture of a variety of variables and demographics of the United States, including location, level of educational attainment, employment information, and potentially even income. Our goal is to have a variety of variables such as these to help us get a better understanding of the disparities faced by U.S. citizens, and women in particular. We have reached the point where our data is mostly clean, but have not actually merged the data frames together yet. One example of a set of data points that we would like to combine is educational attainment data for each gender from the ACS survey that spans from a high school education to a completed Bachelor’s degree with data from the World Bank pertaining to Masters and Doctorate degrees. Some initial findings we found, through our cleaned dataset, is that there is a huge division of the percentage of people getting education among different races. For example, the category “white alone” and “asian alone” shows a higher percentage of education rate compared to the category “black alone.” This will allow us to examine the gap in educational attainment between genders on all levels of the system, and may help us to draw interesting conclusions."
  },
  {
    "objectID": "posts/Blog-post-7/blog-post-7.html",
    "href": "posts/Blog-post-7/blog-post-7.html",
    "title": "Team 8 Final Project",
    "section": "",
    "text": "Blog Post 7\n04/29/2024\nWe are in a pretty good spot for finishing up the project this week. We have finally narrowed our data down to two, highly detailed data sets. We are examining how the disparity that women face, primarily in the workplace, is reflected across different racial groups in the United States. We have our data cleaned and in a state where it can be merged easily. Our primary variables which we are focusing on are income and educational attainment of a bachelors degree. We also plan to use further ACS data to create “heat maps” of the United States to highlight disparities such as the proportion of citizens living above the poverty line by state. All that remains is for us to put this data into digestible figures and tables and decide what regression technique we would like to use. To us, the most appealing regression techniques are a Time-Series regression model and a categorical regression model. We plan to discuss this further and troubleshoot these ideas before our final submission."
  }
]